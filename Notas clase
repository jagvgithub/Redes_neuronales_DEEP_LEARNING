# clase 1
# 4 entregas actividades.


# 24/09/2024

Se ajustarán las fechas de las actividades.

- Problemas de regresión y clasificación 
(modelo de predicción - combinación (fontera lineal) lineal de multiples variables)
cuando no son lineales se hace necesario usar redes neuronales.

Capa de entrada - capas ocultas - capa de salida
Cada neurona tiene unos pesos

- Función de activación

- Backward propagation (prueba y feedback) # tetester que tan bien está prediciendo la red.
sirve para determinar el error y así calibrar la red
ajustando los pesos para que la predicción sea mejor.

- Regla de la cadena.
- tasa de aprendizaje (learning rate)

- back propagation (ajuste de pesos)

A medida que las redes neuronales los costos computacionales son más complejos 

######### terminos importantes 

- forward, backward propagation,parámetros,pesos,regla de la cadena, descenso del gradiente.


Forward Propagation (Propagación Hacia Adelante): Es el proceso de pasar la entrada a través de la red neuronal para obtener una predicción. 
En cada capa, se aplican pesos y funciones de activación a los datos, transformando la entrada hasta llegar a la capa de salida.

Backward Propagation (Propagación Hacia Atrás): Es el proceso de ajustar los pesos de la red después de calcular el error entre la predicción y el valor real. 
Se utiliza para minimizar el error a través de la regla de la cadena, calculando los gradientes y actualizando los pesos en la dirección opuesta al gradiente.

Parámetros: Son las variables que la red neuronal ajusta durante el entrenamiento para aprender a hacer predicciones. Incluyen los pesos y los sesgos 
que se aplican a las entradas en cada neurona.

Pesos: Son los coeficientes que determinan la influencia de cada entrada en la salida de una neurona. Los pesos son actualizados durante 
el entrenamiento para mejorar la precisión de las predicciones.

Regla de la Cadena: Es una fórmula en cálculo que permite calcular la derivada de funciones compuestas. 
En el contexto de redes neuronales, se usa durante la propagación hacia atrás para calcular cómo un cambio en los pesos afecta al error final.

Descenso del Gradiente: Es un algoritmo de optimización utilizado para minimizar la función de pérdida. 
Consiste en ajustar los pesos en la dirección opuesta al gradiente de la función de pérdida, buscando así los valores óptimos de los
parámetros que reducen el error de predicción.

AND lineal
XOR no lineal 


TIPOS de DESCENSO DEL GRADIENTE
- stocastic gradient descent: size 1
- Mini- batch SGD : size >1 < tamaño del training set
- Batch SGD: batch size == tamaño del trainig set

Stochastic Gradient Descent (SGD): Este método actualiza los pesos utilizando solo un ejemplo (tamaño de lote de 1) en cada iteración. 
Esto hace que las actualizaciones sean muy rápidas, pero también introduce variabilidad, ya que cada ejemplo puede llevar a una dirección
diferente en el espacio de parámetros.

Mini-Batch Gradient Descent: En este enfoque, se utiliza un pequeño grupo de ejemplos (batch) para actualizar los pesos. 
El tamaño del batch es mayor que 1 pero menor que el tamaño total del conjunto de entrenamiento. Esto proporciona un buen 
equilibrio entre la rapidez del SGD y la estabilidad del Batch Gradient Descent.

Batch Gradient Descent: Aquí, se utilizan todos los ejemplos del conjunto de entrenamiento (batch size igual al tamaño del conjunto de entrenamiento)
para calcular la actualización de los pesos. Aunque es más estable y proporciona una dirección más precisa para la actualización, puede ser lento 
y consumir mucha memoria, especialmente con conjuntos de datos grandes.

Descenso del gradiente : OPTIMIZADOR.

############## FUNCIONES DE ACTIVACION #############

- RELU : valor negativo de salida, el valor con la función toma cero
- SIGMOIDE : acota el rango de salida entre cero y uno
- SOFTMAX
- TANH

ReLU (Rectified Linear Unit)
Definición: La función ReLU es definida como 
Esto significa que para cualquier valor de entrada negativo, la salida es cero; para valores positivos, la salida es igual al valor de entrada.

Ventajas:

Computacionalmente eficiente (no requiere operaciones exponenciales).
Ayuda a mitigar el problema del desvanecimiento del gradiente, permitiendo que los gradientes fluyan más fácilmente.

Desventajas:

Puede sufrir del problema "dropped neurons", donde ciertas neuronas dejan de activarse completamente (salida siempre cero) durante el entrenamiento.
No está acotada, lo que puede llevar a salidas muy grandes.

2. Sigmoide
Definición: La función sigmoide se define como 
Mapea cualquier valor real a un rango entre 0 y 1.

Ventajas:

Buena para modelar probabilidades y resultados binarios.
Su salida está acotada, lo que hace que sea fácil interpretar como probabilidad.
Desventajas:

Puede sufrir del problema del desvanecimiento del gradiente, lo que dificulta el entrenamiento de redes profundas.
La salida no está centrada en cero, lo que puede causar problemas de convergencia.
3. Softmax
Definición: La función Softmax se usa principalmente en la capa de salida de modelos de clasificación multiclase. Convierte un vector de valores reales en probabilidades que suman 1, usando la fórmula 

Ventajas:

Ideal para clasificación multiclase, ya que produce probabilidades que son interpretables.
Ayuda a seleccionar la clase con la mayor probabilidad al final del modelo.
Desventajas:

Puede ser sensible a valores extremos, lo que puede afectar la estabilidad numérica.
No es adecuada para problemas que no requieren una salida de probabilidad.
4. Tanh (Tangente Hiperbólica)
Definición: La función tanh se define como 

 . Mapea cualquier valor real a un rango entre -1 y 1.

Ventajas:

Está centrada en cero, lo que puede acelerar la convergencia.
Menos susceptible al problema del desvanecimiento del gradiente en comparación con la sigmoide.
Desventajas:

Aún puede sufrir el problema del desvanecimiento del gradiente para valores extremos.
La computación puede ser más costosa que ReLU, ya que implica operaciones exponenciales.

#### 26/09/2024

URL API

keras2 api documentation
pytorch
tensorFlow

# 41 minutos  construcción de la primera red neuronal.
 Cuando se tienen problemas de clasificación de dos clases se recomienda la activación sigmoide,
cuando se tiene un problema de clasificación multiclase se recomienda activación softmax

OVerfitting se puede manejar por medio del # de neuronas dentro de cada capa.
en DEEP LEARNING cuando se manejan millones de datos el test usualmente es el 1%



######################### 01/10/2024 ###################################

Funciones de activación 
Ejemplo de TEXTO
Funciones de activación 

SIGMOIDE: se adpata bien a problemas de clasificacion binaria.
TANGENCIAL : tanh : adecuada cuando hay una clase mayoritaria.
RELU : es raro ver relu en capas de salida, pero sin en capas intermedias.

REGULARIZACION : técnicas para evitar el sobreajuste.
LOGITS : valores de salida reales
PROBITS : valores de salida en probabilidad (haciendo uso de softmax)

tipos de error 

REGRESION : MSE
CLASSIFICATION multiclase : "categorical crossentrophy"
CLASIFICACION binaria : "binary cross-entrophy"

multi etiqueta (multi label - multi task)

REGULARIZACION L1,L2 (para evitar el overfitting)
controlar que la red no memorice las entradas, sino que realmente logre clasificar bien.

DROPOUT : dificultar el aprendixaje de la red para que no memorice 
determinar si se habilita o se desabilita una neurona (NO aplicar en la capa de salida, si en las capas intermedias)
BACTH NORMALIZATION : normalizar capa capa entre 0 1

** las capas densas añaden un crecimiento exponencial en el numero de parámetros
** los valores altos de entrada son un suicidio.

aprendizaje automatico: evita que el criterio experto no tenga que estar en constante monitoreo.


## se implementaran otro tipo de capas para tratar de solucionar el crecimiento exponencial de parámetros



##################################### 03/10/2024 ########################################################

Procesado digital de imagen != DEEP VISION

Perceptrón multicapa : supone un problema en el crecimiento exponencial de los hiperparámetros.

para evitar lo anterior se hace necesario recurrir a CNN redes neuronales convolucionales. 
- Dense layer aprenden patrones globales (en toda la imagen)
- Convoluciones detectan patrones locales (resistente a traslaciones)
- Convoluciones permiteb aorender jeraquias de patrones (patrones locales, como lineas, curvas,etc, a circulos, rectangulos, a constelaciones)

Padding y striding

padding (añadir filas y columnas para hacer posible el análisis)

striding(dar saltos )

# reducción de tamaño

Pooling 
- reducir dimensionalidad espacial
- reducir el # de parámetros
 * lo más habitual es ver poling 2x2

modulos 
* models
* layers de tensorfow.keras(Dense,Conv2D)

la funcion summary() nos muestra paso a paso el # de hiperparametros 
mediante keras.

para evitar el overfitting se debe tener en cuanta el concepto de data augmentation (transformar los datos 
originales, para que el modelo no memorice los datos de train)

Revisar el estado del arte para no empezar los proyectos de cero (transfer learning)

FINE TUNING




########################################### 15/10/2024 ##############################################################


## API dataset actividad a entregar 10/11/2024

import kagglehub

# Download latest version
path = kagglehub.dataset_download("misrakahmed/vegetable-image-dataset")

print("Path to dataset files:", path)

Separacion de dataset en entrenamiento,test y validacion

CONSTRUCCION DE UNA RED CNN

* se divide en 255 para dejar la informacion entre 0 1 (especie de noramlizacion estandar)
pero el ser imagenes no es posible sacar estadisticas, media y sd.

* arquitectura preceptrón multicapa
* arquitectura redes CNN

arquitecturas-
- ad
- model
- bloque















